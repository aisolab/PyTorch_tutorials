{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic concept of torchtext\n",
    "`Torchtext` takes a declarative approach to loading its data\n",
    "* reference: [A Comprehensive Introduction to Torchtext](http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "\n",
    "![Alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/02/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-02-07-10.32.59.png?resize=1024%2C481)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from mecab import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path().cwd() / 'data'\n",
    "nsmc = pd.read_csv(next(data_dir.iterdir()), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0점은 없나요?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제발 시즌2 ㅜㅜ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  label\n",
       "0  애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...      1\n",
       "1                    여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.      0\n",
       "2                        쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ      0\n",
       "3                                        0점은 없나요?...      0\n",
       "4                                          제발 시즌2 ㅜㅜ      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field\n",
    "- api guide : https://torchtext.readthedocs.io/en/latest/data.html#field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "sentence_field = Field(sequential=True, use_vocab=True, tokenize=MeCab().morphs, batch_first=True, fix_length=32)\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, unk_token=None, pad_token=None,\n",
    "                    is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'batch_first',\n",
       " 'build_vocab',\n",
       " 'dtype',\n",
       " 'dtypes',\n",
       " 'eos_token',\n",
       " 'fix_length',\n",
       " 'ignore',\n",
       " 'include_lengths',\n",
       " 'init_token',\n",
       " 'is_target',\n",
       " 'lower',\n",
       " 'numericalize',\n",
       " 'pad',\n",
       " 'pad_first',\n",
       " 'pad_token',\n",
       " 'postprocessing',\n",
       " 'preprocess',\n",
       " 'preprocessing',\n",
       " 'process',\n",
       " 'sequential',\n",
       " 'stop_words',\n",
       " 'tokenize',\n",
       " 'tokenizer_args',\n",
       " 'truncate_first',\n",
       " 'unk_token',\n",
       " 'use_vocab',\n",
       " 'vocab_cls']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sentence_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> None\n",
      "<unk> None\n",
      "None None\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(sentence_field.pad_token, label_field.pad_token)\n",
    "print(sentence_field.unk_token, label_field.unk_token)\n",
    "print(sentence_field.eos_token, label_field.eos_token)\n",
    "print(sentence_field.init_token, label_field.init_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보다 훨 낮다. 1\n"
     ]
    }
   ],
   "source": [
    "example_sentence, example_label = nsmc.iloc[0].tolist()\n",
    "print(example_sentence, example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "list_of_tokens = sentence_field.tokenize(example_sentence)\n",
    "print(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.']\n",
      "False 1\n"
     ]
    }
   ],
   "source": [
    "print(sentence_field.sequential, sentence_field.preprocess(example_sentence))\n",
    "print(label_field.sequential, label_field.preprocess(example_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/vocab.html#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.'], ['여전히', '반복', '되', '고', '있', '는', '80', '년', '대', '한국', '멜', '로', '영화', '의', '유치', '함', '.']]\n"
     ]
    }
   ],
   "source": [
    "list_of_tokenized = nsmc['document'].apply(sentence_field.tokenize).tolist()\n",
    "print(list_of_tokenized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = Counter(itertools.chain.from_iterable(list_of_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counter=count_tokens, min_freq=10)\n",
    "sentence_field.vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_default_unk_index',\n",
       " 'extend',\n",
       " 'freqs',\n",
       " 'itos',\n",
       " 'load_vectors',\n",
       " 'set_vectors',\n",
       " 'stoi',\n",
       " 'unk_index',\n",
       " 'vectors']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.']\n",
      "['<unk>', '<pad>', '.', '이', '는'] 8775\n",
      "tensor([[ 178,   20,  427,    8,   22,  289,  106,   22,   20,   12,   99,  623,\n",
      "           65,  683,   28,   26, 2402,  664,  833,  130,    4, 1161,  178,   20,\n",
      "            3,  753,   20,  105, 1278,  261,    6,    2]])\n"
     ]
    }
   ],
   "source": [
    "print(list_of_tokens)\n",
    "print(sentence_field.vocab.itos[:5], len(sentence_field.vocab))\n",
    "print(sentence_field.numericalize([list_of_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_field.vocab = None # reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_field.build_vocab(list_of_tokenized, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.']\n",
      "['<unk>', '<pad>', '.', '이', '는'] 8775\n",
      "tensor([[ 178,   20,  427,    8,   22,  289,  106,   22,   20,   12,   99,  623,\n",
      "           65,  683,   28,   26, 2402,  664,  833,  130,    4, 1161,  178,   20,\n",
      "            3,  753,   20,  105, 1278,  261,    6,    2]])\n"
     ]
    }
   ],
   "source": [
    "print(list_of_tokens)\n",
    "print(sentence_field.vocab.itos[:5], len(sentence_field.vocab))\n",
    "print(sentence_field.numericalize([list_of_tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/data.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.'] 1\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Example\n",
    "\n",
    "# generate an Example\n",
    "example = Example.fromlist(nsmc.iloc[0].tolist(), fields=[('document', sentence_field), ('label', label_field)])\n",
    "print(example.document, example.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'document',\n",
       " 'fromCSV',\n",
       " 'fromJSON',\n",
       " 'fromdict',\n",
       " 'fromlist',\n",
       " 'fromtree',\n",
       " 'label']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/data.html#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of Examples\n",
    "list_of_examples = [Example.fromlist(row.tolist(),\n",
    "                    fields=[('document', sentence_field), ('label', label_field)]) for _, row in nsmc.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<torchtext.data.example.Example object at 0x7f808fc8a978>,\n",
      " <torchtext.data.example.Example object at 0x7f808fc8a208>,\n",
      " <torchtext.data.example.Example object at 0x7f808fc8a9b0>,\n",
      " <torchtext.data.example.Example object at 0x7f808fc8a9e8>,\n",
      " <torchtext.data.example.Example object at 0x7f808fc8aa20>]\n",
      "['애', '들', '욕', '하', '지', '마', '라', '지', '들', '은', '뭐', '그렇게', '잘', '났', '나', '?', '솔', '까', '거기', '나오', '는', '귀여운', '애', '들', '이', '당신', '들', '보다', '훨', '낮', '다', '.'] 1\n"
     ]
    }
   ],
   "source": [
    "pprint(list_of_examples[:5])\n",
    "print(list_of_examples[0].document, list_of_examples[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "dataset = Dataset(examples=list_of_examples, fields=[('document', sentence_field), ('label', label_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f808fc8a978>,\n",
       " <torchtext.data.example.Example at 0x7f808fc8a208>,\n",
       " <torchtext.data.example.Example at 0x7f808fc8a9b0>,\n",
       " <torchtext.data.example.Example at 0x7f808fc8a9e8>,\n",
       " <torchtext.data.example.Example at 0x7f808fc8aa20>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.examples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': <torchtext.data.field.Field at 0x7f80eefe7b00>,\n",
       " 'label': <torchtext.data.field.Field at 0x7f80eefe7a90>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'download',\n",
       " 'examples',\n",
       " 'fields',\n",
       " 'filter_examples',\n",
       " 'sort_key',\n",
       " 'split',\n",
       " 'splits']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TabularDataset\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/data.html#tabulardataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# using TabularDataset\n",
    "sentence_field = Field(sequential=True, use_vocab=True, tokenize=MeCab().morphs, batch_first=True, fix_length=32)\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, unk_token=None, pad_token=None,\n",
    "                    is_target=True)\n",
    "\n",
    "dataset = TabularDataset(path='data/train.txt', format='TSV', fields=[('document', sentence_field),\n",
    "                                                                      ('label', label_field)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_field.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/data.html#batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mb = Batch(data=dataset.examples[:32], dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.document]:[torch.LongTensor of size 32x32]\n",
       "\t[.label]:[torch.LongTensor of size 32]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get_field_values',\n",
       " 'batch_size',\n",
       " 'dataset',\n",
       " 'document',\n",
       " 'fields',\n",
       " 'fromvars',\n",
       " 'input_fields',\n",
       " 'label',\n",
       " 'target_fields']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(example_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "dict_keys(['document', 'label'])\n",
      "['document'] ['label']\n",
      "tensor([[ 178,   20,  427,  ...,  261,    6,    2],\n",
      "        [1312, 1468,   57,  ...,    1,    1,    1],\n",
      "        [5697,  141, 7411,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [  48,   48,  114,  ...,    1,    1,    1],\n",
      "        [  77,  164,   10,  ...,    1,    1,    1],\n",
      "        [ 168,    1,    1,  ...,    1,    1,    1]]) tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(example_mb.batch_size)\n",
    "print(example_mb.fields)\n",
    "print(example_mb.input_fields, example_mb.target_fields)\n",
    "print(example_mb.document, example_mb.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator\n",
    "* api guide : https://torchtext.readthedocs.io/en/latest/data.html#iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = Iterator(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_iterations_this_epoch',\n",
       " '_random_state_this_epoch',\n",
       " '_restored_from_state',\n",
       " 'batch_size',\n",
       " 'batch_size_fn',\n",
       " 'create_batches',\n",
       " 'data',\n",
       " 'dataset',\n",
       " 'device',\n",
       " 'epoch',\n",
       " 'init_epoch',\n",
       " 'iterations',\n",
       " 'load_state_dict',\n",
       " 'random_shuffler',\n",
       " 'repeat',\n",
       " 'shuffle',\n",
       " 'sort',\n",
       " 'sort_key',\n",
       " 'sort_within_batch',\n",
       " 'splits',\n",
       " 'state_dict',\n",
       " 'train']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mb, y_mb = next(iter(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3103,  2134,    50,   115,   782,   812,  4570,    11,   127,    15,\n",
      "             4,    40,     9,  1477,   489,     2,     2,     3,  3673,     9,\n",
      "            35,   326,    40,    46,   158,     3,  4133,    14,  1277,  4843,\n",
      "           891,     2],\n",
      "        [   67,    11, 33990,  1493,   319,  5346,  9831,     4,   117,   353,\n",
      "            78,     6,   586,   832,  5020,   195,   508,   755,    14,   960,\n",
      "         36372,   262,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(x_mb, y_mb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
